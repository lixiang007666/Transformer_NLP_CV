{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST image classification with Swin Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from keras_vision_transformer import swin_layers\n",
    "from keras_vision_transformer import transformer_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset contains handwritten digits as gray-scale images with pixel sizes of 28-by-28. The pixel values are converted to float numbers and normalized with minimum-maximum scaling. The dataset is labeled with ten categories, represents digits of 0-9.\n",
    "\n",
    "A supervised image classification problem is proposed to demonstrate the application of the Swin Transformer. By taking preprocessed grayscale images as inputs, the Swin Transformer is trained to classify the ten image labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Swin Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S. and Guo, B., 2021. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030. https://arxiv.org/abs/2103.14030"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simplified Swin Transformer configuration is applied with a patch embedding layer, two transformer blocks, and a patch merging layer. Global average pooling and softmax output activations are applied after Swin Transformer blocks.\n",
    "\n",
    "See Liu et al. (2021) for more complicated architecture variants.\n",
    "\n",
    "The hyperparameters of the Swin Transfor are listed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (28, 28, 1) # The image size of the MNIST\n",
    "patch_size = (2, 2) # Segment 28-by-28 frames into 2-by-2 sized patches, patch contents and positions are embedded\n",
    "n_labels = 10 # MNIST labels\n",
    "\n",
    "# Dropout parameters\n",
    "mlp_drop_rate = 0.01 # Droupout after each MLP layer\n",
    "attn_drop_rate = 0.01 # Dropout after Swin-Attention\n",
    "proj_drop_rate = 0.01 # Dropout at the end of each Swin-Attention block, i.e., after linear projections\n",
    "drop_path_rate = 0.01 # Drop-path within skip-connections\n",
    "\n",
    "# Self-attention parameters \n",
    "# (Fixed for all the blocks in this configuration, but can vary per block in larger architectures)\n",
    "num_heads = 8 # Number of attention heads\n",
    "embed_dim = 64 # Number of embedded dimensions\n",
    "num_mlp = 256 # Number of MLP nodes\n",
    "qkv_bias = True # Convert embedded patches to query, key, and values with a learnable additive value\n",
    "qk_scale = None # None: Re-scale query based on embed dimensions per attention head # Float for user specified scaling factor\n",
    "\n",
    "# Shift-window parameters\n",
    "window_size = 2 # Size of attention window (height = width)\n",
    "shift_size = window_size // 2 # Size of shifting (shift_size < window_size)\n",
    "\n",
    "num_patch_x = input_size[0]//patch_size[0]\n",
    "num_patch_y = input_size[1]//patch_size[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input section\n",
    "IN = Input(input_size)\n",
    "X = IN\n",
    "\n",
    "# Extract patches from the input tensor\n",
    "X = transformer_layers.patch_extract(patch_size)(X)\n",
    "\n",
    "# Embed patches to tokens\n",
    "X = transformer_layers.patch_embedding(num_patch_x*num_patch_y, embed_dim)(X)\n",
    "\n",
    "# -------------------- Swin transformers -------------------- #\n",
    "# Stage 1: window-attention + Swin-attention + patch-merging\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        shift_size_temp = 0\n",
    "    else:\n",
    "        shift_size_temp = shift_size\n",
    "\n",
    "    X = swin_layers.SwinTransformerBlock(dim=embed_dim, num_patch=(num_patch_x, num_patch_y), num_heads=num_heads, \n",
    "                             window_size=window_size, shift_size=shift_size_temp, num_mlp=num_mlp, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                             mlp_drop=mlp_drop_rate, attn_drop=attn_drop_rate, proj_drop=proj_drop_rate, drop_path_prob=drop_path_rate, \n",
    "                             name='swin_block{}'.format(i))(X)\n",
    "# Patch-merging\n",
    "#    Pooling patch sequences. Half the number of patches (skip every two patches) and double the embedded dimensions\n",
    "X = transformer_layers.patch_merging((num_patch_x, num_patch_y), embed_dim=embed_dim, name='down{}'.format(i))(X)\n",
    "\n",
    "# ----------------------------------------------------------- #\n",
    "\n",
    "# Convert embedded tokens (2D) to vectors (1D)\n",
    "X = GlobalAveragePooling1D()(X)\n",
    "\n",
    "# The output section\n",
    "OUT = Dense(n_labels, activation='softmax')(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model = keras.models.Model(inputs=[IN,], outputs=[OUT,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "patch_extract (patch_extract (None, 196, 4)            0         \n",
      "_________________________________________________________________\n",
      "patch_embedding (patch_embed (None, 196, 64)           12864     \n",
      "_________________________________________________________________\n",
      "swin_transformer_block (Swin (None, 196, 64)           50072     \n",
      "_________________________________________________________________\n",
      "swin_transformer_block_1 (Sw (None, 196, 64)           50856     \n",
      "_________________________________________________________________\n",
      "patch_merging (patch_merging (None, 49, 128)           32768     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 147,850\n",
      "Trainable params: 147,034\n",
      "Non-trainable params: 816\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient clipping is applied to prevent gradient explosion.\n",
    "\n",
    "Note: the traning of this example is not systematic, and is provided for illustration purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1688/1688 [==============================] - 18s 9ms/step - loss: 0.9973 - accuracy: 0.6846 - val_loss: 0.2451 - val_accuracy: 0.9240\n",
      "Epoch 2/5\n",
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.2827 - accuracy: 0.9116 - val_loss: 0.1642 - val_accuracy: 0.9520\n",
      "Epoch 3/5\n",
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.2192 - accuracy: 0.9317 - val_loss: 0.1936 - val_accuracy: 0.9408\n",
      "Epoch 4/5\n",
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.1768 - accuracy: 0.9469 - val_loss: 0.1270 - val_accuracy: 0.9620\n",
      "Epoch 5/5\n",
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.1627 - accuracy: 0.9491 - val_loss: 0.1053 - val_accuracy: 0.9705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b391819b208>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "opt = keras.optimizers.Adam(learning_rate=1e-4, clipvalue=0.5)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=opt, metrics=['accuracy',])\n",
    "\n",
    "# Training\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_train[:10, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.6543670e-07, 1.8593454e-07, 1.4957112e-05, 8.7632787e-01,\n",
       "       3.2536024e-10, 1.2359164e-01, 1.2831727e-09, 1.5645363e-05,\n",
       "       3.7107464e-05, 1.2507793e-05], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be saved as `model.save()`, but it contains python objects that are not part of the `tensorflow.keras`. Thus when loading the model, it is preferred to load the weights only, and freeze them within a new configuration.\n",
    "\n",
    "e.g.\n",
    "\n",
    "```python\n",
    "weights = dummy_loader(model_old_path)\n",
    "model_new = swin_transformer_model(...)\n",
    "model_new.set_weights(weights)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
